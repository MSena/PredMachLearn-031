## The strategy I persued involved looking out for the relevant covariates in the testing dataset
## (no point in considering null variance covariates in the classification algorithm...), as
## there are lots of variables that don't have any information.
## In order to avoid overfitting in the classification algorithm, I disregard
## the following variables: "user_name";"raw_timestamp_part_1"; "raw_timestamp_part_2";
## "cvtd_timestamp"; "num_window". I also disregard counters "X" and "problem_id"
library(caret)
nsv_ts <- nearZeroVar(testing, saveMetrics=TRUE)
nsv_ts
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
summary(covariates)
#remove spurious variables from covariates
covariates <- covariates[7:58]
summary(covariates)
# add predictive variable to list of variables
covariates[length(covariates)+1] <- "classe"
trainWork <- training[,covariates]
testWork <- testing[,covariates[1:(length(covariates)-1)]]
testWork$classe <- NA
## Cleaning Data
sapply(trainWork, function(x) sum(is.na(x)))
#plot covariates for skewness and normality check
for(i in 1:dim(trainWork)[2]){hist(as.numeric(trainWork[,i]), main = covariates[i])}
#standardizing the variables and checking normality
preObj <- preProcess(trainWork[,-53], method = c("center","scale"))
trainSt <- predict(preObj, trainWork[,-53])
trainSt$classe <- trainWork$classe
summary(trainSt)
inTrain = createDataPartition(y = trainSt$classe, p = 0.6, list = FALSE)
trainCDP = trainSt[ inTrain,]
testCDP = trainSt[-inTrain,]
modFitRf <- train(classe ~ ., method = "rf", data = trainCDP)
saveRDS(modFitRf, file="modFitRandom.rds"); ##myVariableName = readRDS("myFile.rds")
modFitGmb <- train(classe ~ ., method = "gbm", data = trainCDP, verbose = F)
saveRDS(modFitGbm, file = "modFitGbm.rds")
modFitGbm <- modFitGmb
saveRDS(modFitGbm, file = "modFitGbm.rds")
modFitRf
names(modFirRf)
names(modFitRf)
?train
library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
modelFit <- train(type ~.,data=training, method="glm")
args(train.default)
args(trainControl)
set.seed(1235)
modelFit2 <- train(type ~.,data=training, method="glm")
modelFit2
set.seed(1235)
modelFit3 <- train(type ~.,data=training, method="glm")
modelFit3
library(mlbench)
data(Sonar)
str(Sonar[, 1:10])
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining,]
testing  <- Sonar[-inTraining,]
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining,]
testing  <- Sonar[-inTraining,]
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
set.seed(825)
gbmFit1 <- train(Class ~ ., data = training,
method = "gbm",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE)
gbmFit1
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
n.trees = (1:30)*50,
shrinkage = 0.1,
n.minobsinnode = 20)
nrow(gbmGrid)
set.seed(825)
gbmFit2 <- train(Class ~ ., data = training,
method = "gbm",
trControl = fitControl,
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = gbmGrid)
gbmFit2
trellis.par.set(caretTheme())
plot(gbmFit2)
trellis.par.set(caretTheme())
plot(gbmFit2, metric = "Kappa")
trellis.par.set(caretTheme())
plot(gbmFit2, metric = "Kappa", plotType = "level",
scales = list(x = list(rot = 90)))
ggplot(gbmFit2)
fitControl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
## Estimate class probabilities
classProbs = TRUE,
## Evaluate performance using
## the following function
summaryFunction = twoClassSummary)
set.seed(825)
gbmFit3 <- train(Class ~ ., data = training,
method = "gbm",
trControl = fitControl,
verbose = FALSE,
tuneGrid = gbmGrid,
## Specify which metric to optimize
metric = "ROC")
gbmFit3
whichTwoPct <- tolerance(gbmFit3$results, metric = "ROC",
tol = 2, maximize = TRUE)
cat("best model within 2 pct of best:\n")
gbmFit3$results[whichTwoPct,1:6]
whichTwoPct <- tolerance(gbmFit3$results, metric = "ROC",
tol = 2, maximize = TRUE)
cat("best model within 2 pct of best:"\n)
cat("best model within 2 pct of best:\"n)
whichTwoPct <- tolerance(gbmFit3$results, metric = "ROC",
tol = 2, maximize = TRUE)
cat("best model within 2 pct of best:\n")
predict(gbmFit3, newdata = head(testing))
predict(gbmFit3, newdata = head(testing), type = "prob")
#install.packages("doParallel")
library(doParallel)
registerDoParallel(cores=2)
training <- read.table("./projData/training.csv", sep = ",", header = TRUE)
dim(training)
View(training)
library(caret)
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
summary(covariates)
#remove spurious variables from covariates
covariates <- covariates[7:58]
summary(covariates)
# add predictive variable to list of variables
covariates[length(covariates)+1] <- "classe"
nsv_ts <- nearZeroVar(testing, saveMetrics=TRUE)
nsv_ts
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
summary(covariates)
#remove spurious variables from covariates
covariates <- covariates[7:58]
summary(covariates)
# add predictive variable to list of variables
covariates[length(covariates)+1] <- "classe"
##Load testing data
testing <- read.table("./projData/testing.csv", sep = ",", header = TRUE)
dim(testing)
View(testing)
nsv_ts <- nearZeroVar(testing, saveMetrics=TRUE)
nsv_ts
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
summary(covariates)
#remove spurious variables from covariates
covariates <- covariates[7:58]
summary(covariates)
# add predictive variable to list of variables
covariates[length(covariates)+1] <- "classe"
trainWork <- training[,covariates]
testWork <- testing[,covariates[1:(length(covariates)-1)]]
testWork$classe <- NA
## Cleaning Data
sapply(trainWork, function(x) sum(is.na(x)))
#standardizing the variables and checking normality
preObj <- preProcess(trainWork[,-53], method = c("center","scale"))
trainSt <- predict(preObj, trainWork[,-53])
trainSt$classe <- trainWork$classe
summary(trainSt)
set.seed(12345)
inTrain = createDataPartition(y = trainSt$classe, p = 0.6, list = FALSE)
trainCDP = trainSt[ inTrain,]
testCDP = trainSt[-inTrain,]
## The function trainControl can be used to specifiy the type of resampling
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
## Alternate Tuning Grid
Grid <-  expand.grid(interaction.depth = c(1, 3, 5, 9),
n.trees = (1:15)*100,
shrinkage = 0.1,
n.minobsinnode = 20)
set.seed(123)
modFitGbm2 <- train(classe ~ ., data = trainCDP,
method = "gbm",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = Grid)
##Load training data
training <- read.table("./projData/training.csv", sep = ",", header = TRUE)
dim(training)
View(training)
##Load testing data
testing <- read.table("./projData/testing.csv", sep = ",", header = TRUE)
dim(testing)
View(testing)
## The strategy I persued involved looking out for the relevant covariates in the testing dataset
## (no point in considering null variance covariates in the classification algorithm...), as
## there are lots of variables that don't have any information.
## In order to avoid overfitting in the classification algorithm, I disregard
## the following variables: "user_name";"raw_timestamp_part_1"; "raw_timestamp_part_2";
## "cvtd_timestamp"; "num_window". I also disregard counters "X" and "problem_id"
library(caret)
nsv_ts <- nearZeroVar(testing, saveMetrics=TRUE)
nsv_ts
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
summary(covariates)
#remove spurious variables from covariates
covariates <- covariates[7:58]
summary(covariates)
# add predictive variable to list of variables
covariates[length(covariates)+1] <- "classe"
trainWork <- training[,covariates]
testWork <- testing[,covariates[1:(length(covariates)-1)]]
testWork$classe <- NA
## Cleaning Data
sapply(trainWork, function(x) sum(is.na(x)))
#plot covariates for skewness and normality check
for(i in 1:dim(trainWork)[2]){hist(as.numeric(trainWork[,i]), main = covariates[i])}
#standardizing the variables and checking normality
preObj <- preProcess(trainWork[,-53], method = c("center","scale"))
trainSt <- predict(preObj, trainWork[,-53])
trainSt$classe <- trainWork$classe
summary(trainSt)
##training the model
set.seed(12345)
inTrain = createDataPartition(y = trainSt$classe, p = 0.6, list = FALSE)
trainCDP = trainSt[ inTrain,]
testCDP = trainSt[-inTrain,]
## The function trainControl can be used to specifiy the type of resampling
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
## Alternate Tuning Grid
Grid <-  expand.grid(interaction.depth = c(1, 3, 5, 9),
n.trees = (1:15)*100,
shrinkage = 0.1,
n.minobsinnode = 20)
set.seed(123)
ModFitRf2 <- train(Class ~ ., data = trainCDP,
method = "rf",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = Grid)
ModFitRf2
saveRDS(modFitRf2, file="modFitRandom2.rds")
set.seed(123)
ModFitRf2 <- train(classe ~ ., data = trainCDP,
method = "rf",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = Grid)
ModFitRf2
saveRDS(modFitRf2, file="modFitRandom2.rds")
##Load training data
training <- read.table("./projData/training.csv", sep = ",", header = TRUE)
dim(training)
View(training)
##Load testing data
testing <- read.table("./projData/testing.csv", sep = ",", header = TRUE)
dim(testing)
View(testing)
## The strategy I persued involved looking out for the relevant covariates in the testing dataset
## (no point in considering null variance covariates in the classification algorithm...), as
## there are lots of variables that don't have any information.
## In order to avoid overfitting in the classification algorithm, I disregard
## the following variables: "user_name";"raw_timestamp_part_1"; "raw_timestamp_part_2";
## "cvtd_timestamp"; "num_window". I also disregard counters "X" and "problem_id"
library(caret)
nsv_ts <- nearZeroVar(testing, saveMetrics=TRUE)
nsv_ts
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
summary(covariates)
#remove spurious variables from covariates
covariates <- covariates[7:58]
summary(covariates)
# add predictive variable to list of variables
covariates[length(covariates)+1] <- "classe"
trainWork <- training[,covariates]
testWork <- testing[,covariates[1:(length(covariates)-1)]]
testWork$classe <- NA
## Cleaning Data
sapply(trainWork, function(x) sum(is.na(x)))
#plot covariates for skewness and normality check
for(i in 1:dim(trainWork)[2]){hist(as.numeric(trainWork[,i]), main = covariates[i])}
#standardizing the variables and checking normality
preObj <- preProcess(trainWork[,-53], method = c("center","scale"))
trainSt <- predict(preObj, trainWork[,-53])
trainSt$classe <- trainWork$classe
summary(trainSt)
##training the model
set.seed(12345)
inTrain = createDataPartition(y = trainSt$classe, p = 0.6, list = FALSE)
trainCDP = trainSt[ inTrain,]
testCDP = trainSt[-inTrain,]
?expand.grid
modFitRf <- readRDS(modFitRandom.rds)
getwd()
modFitRf <- readRDS("modFitRandom.rds")
print(modFitRf$finalModel)
predRf <- predict(modFitRf,newdata = testCDP)
confusionMatrix(predRf, testCDP$classe)
cmRf <- confusionMatrix(predRf, testCDP$classe)
names(cmRf)
cmRf$overall
cmRf$dots
cmRf$byClass
cmRf$table
cmRf$positive
cmRf$overall[1]
modFitB <- readRDS("modFitGbm.rds")
predB <- predict(modFitB, newdata = testCDP)
cmB <- confusionMatrix(predB,testCDP$classe)
cmB
## The function trainControl can be used to specifiy the type of resampling
fitControl <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
## repeated ten times
repeats = 10)
Grid <-  expand.grid(interaction.depth = c(1, 3, 5, 9),
n.trees = (1:15)*100,
shrinkage = 0.1,
n.minobsinnode = 20,
mtry = 27)
set.seed(123)
ModFitRf2 <- train(Class ~ ., data = trainCDP,
method = "rf",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = Grid)
ModFitRf2
saveRDS(modFitRf2, file="modFitRandom2.rds")
set.seed(123)
ModFitRf2 <- train(classe ~ ., data = trainCDP,
method = "rf",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = Grid)
ModFitRf2
saveRDS(modFitRf2, file="modFitRandom2.rds")
Grid <-  expand.grid(interaction.depth = c(1, 3, 5, 9),
n.trees = (1:15)*100,
shrinkage = 0.1,
n.minobsinnode = 20,
mtry = 27)
set.seed(123)
ModFitRf2 <- train(classe ~ ., data = trainCDP,
method = "rf",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE,
## Now specify the exact models
## to evaludate:
tuneGrid = Grid)
ModFitRf2
saveRDS(modFitRf2, file="modFitRandom2.rds")
ModFitRf2 <- train(classe ~ ., data = trainCDP,
method = "rf",
trControl = fitControl,
## This last option is actually one
## for gbm() that passes through
verbose = FALSE)
saveRDS(modFitRf2, file="modFitRandom2.rds")
saveRDS(ModFitRf2, file="modFitRandom2.rds")
install.packages("evaluate")
modFit <- readRDS("modFitRandom2.rds")
modFit2 <- readRDS("modFitRandom2.rds")
modfit1 <- readRDS("modFitRandom.rds")
modFit1 <- readRDS("modFitRandom.rds")
pred1 <- predict(modFit1, newdata = testCDP)
confM1 <- confusionMatrix(pred1,testCDP$classe)
confM1 <- confusion.Matrix(pred1,testCDP$classe)
library(caret)
confM1 <- confusionMatrix(pred1,testCDP$classe)
confM1
pred2 <- predict(modFit2, newdata = testCDP)
confM2 <- confusionMatrix(pred2,testCDP$classe)
confM2
names(confM1)
print(confM1$table);print(confM2$table)
names(confM1)
confM1$overall[1]
confM2$overall[1]
names(testWork)
testWork$problem_id <- testing$problem_id
head(testWork)
testing$problem_id
names(testing)
testing <- read.table("./projData/testing.csv", sep = ",", header = TRUE)
##Load training data
training <- read.table("./projData/training.csv", sep = ",", header = TRUE)
dim(training)
##Load testing data
testing <- read.table("./projData/testing.csv", sep = ",", header = TRUE)
dim(testing)
library(caret)
nsv_ts <- nearZeroVar(testing, saveMetrics=TRUE)
nsv_ts
#define relevant covariates
covariates <- rownames(nsv_ts[nsv_ts$zeroVar == 0,])
covariates
#remove spurious variables from covariates
covariates <- covariates[7:58]
covariates
covariates[length(covariates)+1] <- "classe"
trainWork <- training[,covariates]
names(trainWork)
sapply(trainWork, function(x) sum(is.na(x)))
preObj  <- preProcess(trainWork[,-53], method = c("center","scale"))
trainSt <- predict(preObj, trainWork[,-53])
trainSt$classe <- trainWork$classe
summary(trainSt)
covariates
modFit2 <- readRDS("modFitRandom2")
modFit2 <- readRDS("modFitRandom2.rds")
names(testing)
covariates
testWork <- testing[,covariates[1:(length(covariates)-1)]]
testWork$classe <- NA ## the classe variable is not present in the test data set
names(testWork) == covariates
testSt  <- predict(preObj, testWork[,-53])
names(testSd)
names(testSt)
testSt$problem_id <- testing$problem_id
head(testSt,2)
predSubmit <- predict(modFitRf2, newdata = testSt)
predSubmit <- predict(modFit2, newdata = testSt)
predSubmit
predSubmit <- predict(modFit2, newdata = testSt, type = "prob")
predSubmit
apply(predSubmit,1, function(x) which (x == max(x)))
t(apply(predSubmit,1, function(x) which (x == max(x))))
pr <- t(apply(predSubmit,1, function(x) which (x == max(x))))
pr
?cbind
cbind(predSubmit,pr)
cbind(predSubmit,apply(predSubmit,1, function(x) which (x == max(x))))
cbind(predSubmit, classe = apply(predSubmit,1, function(x) which (x == max(x))))
results <- cbind(predSubmit, classe = apply(predSubmit,1, function(x) which (x == max(x))))
results
?ifelse
ifelse(results$classe == "1",results$classe  <- "A",results$classe <- results$classe)
ifelse(results$classe == 1,results$classe  <- "A",results$classe <- results$classe)
trambolho <- Letter(results$classe)
trambolho <- letters(results$classe)
?LETTERS
Trambolho <- LETTERS[results$classe]
Trambolho
Trambolho <- LETTERS[results$classe[1]]
Trambolho
results$classe
results
results <- cbind(predSubmit, classe = apply(predSubmit,1, function(x) which (x == max(x))))
results
results$classe[1]
results$classe[2]
Trambolho <- LETTERS[results$classe[1]]
Trambolho
Trambolho <- LETTERS[results$classe[2]]
Trambolho
results <- cbind(predSubmit, classe = LETTERS[apply(predSubmit,1, function(x) which (x == max(x)))])
results
setwd("C:/Users/msilva/Desktop/Coursera/08_Practical Machine Learning/R_Scripts/PredMachLearn-031")
aaa <- sapply(trainWork, function(x) sum(is.na(x)))
aaa
sum(aaa)
